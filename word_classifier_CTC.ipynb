{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training: Connectionist temporal classification - Word Classification\n",
    "Training of CTC model for words recognition\n",
    "## TODO\n",
    "```\n",
    "Blank labels? Indexs?\n",
    "Add propper accuracy\n",
    "Add border to words images\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.layers import core as layers_core\n",
    "import time\n",
    "import math\n",
    "import unidecode\n",
    "\n",
    "sys.path.append('src')\n",
    "from ocr.datahelpers import load_words_data, corresponding_shuffle, char2idx, sequences_to_sparse\n",
    "from ocr.helpers import img_extend\n",
    "from ocr.mlhelpers import TrainingPlot\n",
    "from ocr.tfhelpers import create_cell\n",
    "from ocr.imgtransform import coordinates_remap\n",
    "from ocr.normalization import image_standardization\n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "plt.rcParams['figure.figsize'] = (9.0, 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANG = 'en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading words...\n",
      " |████████████████████████████████████████| 100.0% \n",
      "-> Number of words: 5069\n"
     ]
    }
   ],
   "source": [
    "images, labels = load_words_data(\n",
    "    ['data/processed/breta/words_gaplines/'], load_gaplines=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_size = 52\n",
    "PAD = 0   # Padding\n",
    "\n",
    "num_new_images = 2                 #  Number of new images per image\n",
    "fac_alpha = 2.0                    # Factors for image preprocessing\n",
    "fac_sigma = 0.08\n",
    "\n",
    "num_buckets = 10\n",
    "slider_size = (60, 2)\n",
    "step_size = 2\n",
    "N_INPUT = slider_size[0]*slider_size[1]\n",
    "vocab_size = char_size + 2         # Number of different chars + <PAD> and <EOS>\n",
    "\n",
    "layers = 2\n",
    "residual_layers = 1        # HAVE TO be smaller than layers\n",
    "units = 256\n",
    "num_hidden = 2*units\n",
    "\n",
    "\n",
    "learning_rate = 1e-4               # 1e-4\n",
    "max_gradient_norm = 5.0            # For gradient clipping\n",
    "dropout = 0.4\n",
    "train_per = 0.8                    # Percentage of training data\n",
    "\n",
    "TRAIN_STEPS = 100000               # Number of training steps!\n",
    "TEST_ITER = 150\n",
    "LOSS_ITER = 50\n",
    "SAVE_ITER = 2000\n",
    "BATCH_SIZE = 20\n",
    "EPOCH = 2000                       # Number of batches in epoch - not accurate\n",
    "save_location = 'models/word-clas/' + LANG + '/CTC/Classifier'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images: 4055\n",
      "Testing images: 1014\n"
     ]
    }
   ],
   "source": [
    "# Shuffle data for later splitting\n",
    "images, labels = corresponding_shuffle([images, labels])\n",
    "\n",
    "for i in range(len(images)):\n",
    "    images[i] = cv2.copyMakeBorder(\n",
    "        images[i],\n",
    "        0, 0, slider_size[1]//2, slider_size[1]//2,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=[0, 0, 0])\n",
    "\n",
    "labels_idx = np.empty(len(labels), dtype=object)\n",
    "for i, label in enumerate(labels):\n",
    "    labels_idx[i] = [char2idx(c) for c in label]    # char2idx(c, True)-2\n",
    "\n",
    "# Split data on train and test dataset\n",
    "div = int(train_per * len(images))\n",
    "\n",
    "trainImages = images[0:div]\n",
    "testImages = images[div:]\n",
    "\n",
    "trainLabels_idx = labels_idx[0:div]\n",
    "testLabels_idx = labels_idx[div:]\n",
    "\n",
    "print(\"Training images:\", div)\n",
    "print(\"Testing images:\", len(images) - div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed train images 12165\n"
     ]
    }
   ],
   "source": [
    "# Dont mix train and test images\n",
    "trainImagesFinal = np.empty(len(trainImages) * (num_new_images+1), dtype=object)\n",
    "trainLabelsFinal_idx = np.empty(len(trainImages)*(num_new_images+1), dtype=object)\n",
    "for idx, img in enumerate(trainImages):\n",
    "    trainImagesFinal[idx*(num_new_images+1)] = img\n",
    "    trainLabelsFinal_idx[idx*(num_new_images+1)] = trainLabels_idx[idx]\n",
    "    for i in range(num_new_images):\n",
    "        trainImagesFinal[idx*(num_new_images+1) + (i+1)] = coordinates_remap(img, fac_alpha, fac_sigma)\n",
    "        trainLabelsFinal_idx[idx*(num_new_images+1) + (i+1)] = trainLabels_idx[idx]\n",
    "        \n",
    "print(\"Transformed train images\", len(trainImagesFinal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BucketDataIterator():\n",
    "    \"\"\" Iterator for feeding seq2seq model during training \"\"\"\n",
    "    def __init__(self,\n",
    "                 images,\n",
    "                 targets,\n",
    "                 num_buckets=5,\n",
    "                 slider=(60, 30),\n",
    "                 slider_step=2,\n",
    "                 train=True):\n",
    "        \n",
    "        self.train = train\n",
    "        \n",
    "        # First PADDING of images to slider size: -(-a // b) ==  ceil(a/b)\n",
    "        self.slider = slider\n",
    "        for i in range(len(images)):\n",
    "            images[i] = image_standardization(img_extend(\n",
    "                images[i],\n",
    "                (images[i].shape[0], max(-(-images[i].shape[1] // slider_step) * slider_step, 60))))\n",
    "        in_length = [(image.shape[1] + 1 - slider[1])//slider_step for image in images]\n",
    "        \n",
    "        # Split images to sequence of vectors\n",
    "        imgseq = np.empty(len(images), dtype=object)\n",
    "        for i, img in enumerate(images):\n",
    "            imgseq[i] = [img[:, loc*slider_step: loc*slider_step + slider[1]].flatten()\n",
    "                         for loc in range(in_length[i])]\n",
    "\n",
    "        # Create pandas dataFrame and sort it by images width (length) \n",
    "        self.dataFrame = pd.DataFrame({'in_length': in_length,\n",
    "                                       'images': imgseq,\n",
    "                                       'targets': targets\n",
    "                                      }).sort_values('in_length').reset_index(drop=True)\n",
    "\n",
    "        bsize = int(len(images) / num_buckets)\n",
    "        self.num_buckets = num_buckets\n",
    "        \n",
    "        # Create buckets by slicing parts by indexes\n",
    "        self.buckets = []\n",
    "        for bucket in range(num_buckets-1):\n",
    "            self.buckets.append(self.dataFrame.iloc[bucket * bsize: (bucket+1) * bsize])\n",
    "        self.buckets.append(self.dataFrame.iloc[(num_buckets-1) * bsize:])        \n",
    "        \n",
    "        self.buckets_size = [len(bucket) for bucket in self.buckets]\n",
    "\n",
    "        # cursor[i] will be the cursor for the ith bucket\n",
    "        self.cursor = np.array([0] * num_buckets)\n",
    "        self.bucket_order = np.random.permutation(num_buckets)\n",
    "        self.bucket_cursor = 0\n",
    "        self.shuffle()\n",
    "        print(\"Iterator created.\")\n",
    "\n",
    "    def shuffle(self, idx=None):\n",
    "        \"\"\" Shuffle idx bucket or each bucket separately \"\"\"\n",
    "        for i in [idx] if idx is not None else range(self.num_buckets):\n",
    "            self.buckets[i] = self.buckets[i].sample(frac=1).reset_index(drop=True)\n",
    "            self.cursor[i] = 0\n",
    "\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        \"\"\"\n",
    "        Creates next training batch of size: batch_size\n",
    "        Retruns: image seq, letter seq,\n",
    "                 image seq lengths, letter seq lengths\n",
    "        \"\"\"\n",
    "        i_bucket = self.bucket_order[self.bucket_cursor]\n",
    "        # Increment cursor and shuffle in case of new round\n",
    "        self.bucket_cursor = (self.bucket_cursor + 1) % self.num_buckets\n",
    "        if self.bucket_cursor == 0:\n",
    "            self.bucket_order = np.random.permutation(self.num_buckets)\n",
    "            \n",
    "        if self.cursor[i_bucket] + batch_size > self.buckets_size[i_bucket]:\n",
    "            self.shuffle(i_bucket)\n",
    "\n",
    "        # Handle too big batch sizes\n",
    "        if (batch_size > self.buckets_size[i_bucket]):\n",
    "            batch_size = self.buckets_size[i_bucket]\n",
    "\n",
    "        res = self.buckets[i_bucket].iloc[self.cursor[i_bucket]:\n",
    "                                          self.cursor[i_bucket]+batch_size]\n",
    "        self.cursor[i_bucket] += batch_size\n",
    "\n",
    "        # PAD input sequence and output\n",
    "        input_max = max(res['in_length'])\n",
    "        \n",
    "        input_seq = np.zeros((batch_size, input_max, N_INPUT), dtype=np.float32)\n",
    "        for i, img in enumerate(res['images']):\n",
    "            try:\n",
    "                input_seq[i][:res['in_length'].values[i]] = img\n",
    "            except:\n",
    "                print(i, \":\", res['in_length'].values[i])\n",
    "                print(img)\n",
    "                \n",
    "        input_seq = input_seq.swapaxes(0, 1)\n",
    "\n",
    "        targets = sequences_to_sparse(res['targets'].values)\n",
    "        \n",
    "        return input_seq, targets, res['in_length'].values\n",
    "    \n",
    "    def next_feed(self, size):\n",
    "        \"\"\" Create feed directly for model training \"\"\"\n",
    "        (inputs_,\n",
    "         targets_,\n",
    "         inputs_length_) = self.next_batch(size)\n",
    "        return {\n",
    "            inputs: inputs_,\n",
    "            inputs_length: inputs_length_,\n",
    "            targets: targets_,\n",
    "            keep_prob: (1.0 - dropout) if self.train else 1.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create iterator for feeding RNN\n",
    "# Create only once, it modifies: labels_idx\n",
    "train_iterator = BucketDataIterator(trainImagesFinal,\n",
    "                                    trainLabelsFinal_idx,\n",
    "                                    num_buckets,\n",
    "                                    slider_size,\n",
    "                                    step_size,\n",
    "                                    train=True)\n",
    "test_iterator = BucketDataIterator(testImages,\n",
    "                                   testLabels_idx,\n",
    "                                   num_buckets,\n",
    "                                   slider_size,\n",
    "                                   step_size,\n",
    "                                   train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input placehodlers\n",
    "# N_INPUT -> size of vector representing one image in sequence\n",
    "# Inputs - Time major: (max_seq_length, batch_size, vec_size)\n",
    "inputs = tf.placeholder(shape=(None, None, N_INPUT),\n",
    "                        dtype=tf.float32,\n",
    "                        name='inputs')\n",
    "inputs_length = tf.placeholder(shape=(None,),\n",
    "                               dtype=tf.int32,\n",
    "                               name='inputs_length')\n",
    "# required for training, not required for testing and application\n",
    "targets = tf.sparse_placeholder(dtype=tf.int32,\n",
    "                                name='targets')\n",
    "\n",
    "# Dropout value\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_cell_fw = create_cell(units,\n",
    "                          layers,\n",
    "                          residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)\n",
    "enc_cell_bw = create_cell(units,\n",
    "                          layers,\n",
    "                          residual_layers,\n",
    "                          is_dropout=True,\n",
    "                          keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CNN ###\n",
    "SCALE = 0.01 # 0.1\n",
    "# Functions for initializing convulation and pool layers\n",
    "def weights(name, shape):\n",
    "    return tf.get_variable(name, shape=shape,\n",
    "                           initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                           regularizer=tf.contrib.layers.l2_regularizer(scale=SCALE))\n",
    "\n",
    "def bias(const, shape, name=None):\n",
    "    return tf.Variable(tf.constant(const, shape=shape), name=name)\n",
    "\n",
    "def conv2d(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME', name=name)\n",
    "\n",
    "def conv2d2(x, W, name=None):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "def max_pool_2x2(x, name=None):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "def inception2d(x, in_channels, filter_count, var_dict):\n",
    "    # 1x1\n",
    "    one_by_one = conv2d(x, var_dict['one_filter']) + var_dict['one_bias']\n",
    "    # 3x3\n",
    "    three_by_three = conv2d(x, var_dict['three_filter']) + var_dict['three_bias']\n",
    "    # 5x5\n",
    "    five_by_five = conv2d(x, var_dict['five_filter']) + var_dict['five_bias']\n",
    "    # avg pooling\n",
    "    pooling = tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "    x = tf.concat([one_by_one, three_by_three, five_by_five, pooling], axis=3)  # Concat in the 4th dim to stack\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "# in_channels1 = 4\n",
    "# filter_count1 = 12\n",
    "\n",
    "# in_channels2 = 40\n",
    "# filter_count2 = 20\n",
    "\n",
    "# W_conv1 = weights('W_conv1', shape=[16, 16, 1, 4])\n",
    "# b_conv1 = bias(0.1, shape=[4], name='b_conv1')\n",
    "# W_fc1 = weights('W_fc2', shape=[8*8*100, char_size])\n",
    "# b_fc1 = bias(0.1, shape=[char_size], name='b_fc2')\n",
    "\n",
    "# var_dict1 = {\n",
    "#     'one_filter': weights('one_filter1',shape=[1, 1, in_channels1, filter_count1]),\n",
    "#     'one_bias': bias(0.2, shape=[filter_count1]),\n",
    "#     'three_filter': weights('three_filter1', shape=[3, 3, in_channels1, filter_count1]),\n",
    "#     'three_bias': bias(0.2, shape=[filter_count1]),\n",
    "#     'five_filter': weights('five_filter1', shape=[5, 5, in_channels1, filter_count1]),\n",
    "#     'five_bias': bias(0.2, shape=[filter_count1])\n",
    "# }\n",
    "\n",
    "# var_dict2 = {\n",
    "#     'one_filter': weights('one_filter2',shape=[1, 1, in_channels2, filter_count2]),\n",
    "#     'one_bias': bias(0.2, shape=[filter_count2]),\n",
    "#     'three_filter': weights('three_filter2', shape=[3, 3, in_channels2, filter_count2]),\n",
    "#     'three_bias': bias(0.2, shape=[filter_count2]),\n",
    "#     'five_filter': weights('five_filter2', shape=[5, 5, in_channels2, filter_count2]),\n",
    "#     'five_bias': bias(0.2, shape=[filter_count2])\n",
    "# }\n",
    "\n",
    "\n",
    "# def CNN(x):\n",
    "#     x = tf.image.per_image_standardization(x)\n",
    "#     img = tf.reshape(x, [1, slider_size[0], slider_size[1], 1])  \n",
    "#     # 1. Layer - Convulation\n",
    "#     h_conv1 = tf.nn.relu(conv2d2(img, W_conv1) + b_conv1, name='h_conv1')    \n",
    "#     # 2. Layer - Max Pool\n",
    "#     h_pool1 = max_pool_2x2(h_conv1, name='h_pool1')\n",
    "#     # 3. Inception\n",
    "#     incept1 = inception2d(h_pool1, in_channels1, filter_count1, var_dict1)\n",
    "#     # 4. Inception\n",
    "#     incept2 = inception2d(incept1, in_channels2, filter_count2, var_dict2)\n",
    "#     # 5. Layer - Max Pool\n",
    "#     h_pool3 = max_pool_2x2(incept2)\n",
    "\n",
    "#     return h_pool3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input images CNN\n",
    "# processed_inputs = tf.map_fn(\n",
    "#     lambda seq: tf.map_fn(\n",
    "#         lambda img:\n",
    "#             tf.reshape(\n",
    "#                 CNN(tf.reshape(img, [slider_size[0], slider_size[1], 1])), [-1]),\n",
    "#         seq),\n",
    "#     inputs,\n",
    "#     dtype=tf.float32)\n",
    "\n",
    "processed_inputs = inputs\n",
    "\n",
    "# Bidirectional RNN, gibe fw and bw outputs separately\n",
    "bi_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw = enc_cell_fw,\n",
    "    cell_bw = enc_cell_bw,\n",
    "    inputs = processed_inputs,\n",
    "    sequence_length = inputs_length,\n",
    "    dtype = tf.float32,\n",
    "    time_major = True)\n",
    "\n",
    "con_outputs = tf.concat(bi_outputs, -1)\n",
    "\n",
    "max_timesteps, batch_size, _ = tf.unstack(tf.shape(inputs))\n",
    "\n",
    "W = weights('W', shape=[num_hidden, vocab_size])\n",
    "b = bias(0., shape=[vocab_size], name='b')\n",
    "\n",
    "outputs = tf.reshape(con_outputs, [-1, num_hidden])\n",
    "logits = tf.matmul(outputs, W) + b\n",
    "logits = tf.reshape(logits, [-1, batch_size, vocab_size])\n",
    "\n",
    "# Or tf.nn.ctc_greedy_decoder (shoudl be faster, but beam_widt=1)\n",
    "decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits, inputs_length)\n",
    "\n",
    "word_prediction = tf.sparse_tensor_to_dense(decoded[0], name='word_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "fd  = test_iterator.next_feed(5)\n",
    "print(word_prediction.eval(fd).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctc_loss = tf.nn.ctc_loss(targets, logits, inputs_length)\n",
    "regularization = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.identity(tf.reduce_mean(ctc_loss) + sum(regularization), name='loss')\n",
    "\n",
    "### Optimization\n",
    "# optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "train_step = optimizer.minimize(loss, name='train_step')\n",
    "\n",
    "test_targets = tf.sparse_tensor_to_dense(targets)\n",
    "\n",
    "# TODO CHANGE TO ACCURACY\n",
    "\n",
    "# It is label error rate not accuracy\n",
    "accuracy = tf.reduce_mean(\n",
    "    tf.edit_distance(tf.cast(decoded[0], tf.int32), targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy + Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_length = decoded[0].get_shape().as_list()[1]\n",
    "# targets_length = targets.get_shape().as_list()[1]\n",
    "\n",
    "# pad_lenght = tf.maximum(pred_length, targets_length)\n",
    "\n",
    "# pred_pad = tf.pad(\n",
    "#     word_prediction,\n",
    "#     [[0, 0],\n",
    "#      [0, pad_lenght - pred_length]],\n",
    "#     constant_values=PAD,\n",
    "#     mode='CONSTANT')\n",
    "# targets_pad = tf.pad(\n",
    "#     test_targets,\n",
    "#     [[0, 0],\n",
    "#      [0, pad_lenght - targets_lenght]],\n",
    "#     constant_values=PAD,\n",
    "#     mode='CONSTANT')\n",
    "\n",
    "# acc_weights = tf.cast(tf.divide(pred_pad, pred_pad), tf.float32)\n",
    "\n",
    "# # acc_weights = tf.sequence_mask(\n",
    "# #     tf.subtract(final_seq_lengths, 1),    # word_inputs_length, try max(targets, inputs)\n",
    "# #     word_pad_lenght,\n",
    "# #     dtype=tf.float32)\n",
    "\n",
    "# correct_prediction = tf.equal(pred_pad, targets_pad)\n",
    "# accuracy = (tf.reduce_sum(tf.cast(correct_prediction, tf.float32) * acc_weights) \\\n",
    "#             / tf.reduce_sum(acc_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Creat plot for live stats ploting\n",
    "trainPlot = TrainingPlot(TRAIN_STEPS, TEST_ITER, LOSS_ITER)\n",
    "\n",
    "try:\n",
    "    for i_batch in range(TRAIN_STEPS):\n",
    "        fd = train_iterator.next_feed(BATCH_SIZE)\n",
    "        train_step.run(fd)\n",
    "        break\n",
    "        \n",
    "        if i_batch % LOSS_ITER == 0:\n",
    "            # Plotting loss\n",
    "            tmpLoss = loss.eval(fd)\n",
    "            trainPlot.updateCost(tmpLoss, i_batch // LOSS_ITER)\n",
    "    \n",
    "        if i_batch % TEST_ITER == 0:\n",
    "            # Plotting accuracy\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            accTest = accuracy.eval(fd_test)\n",
    "            accTrain = accuracy.eval(fd)\n",
    "            trainPlot.updateAcc(accTest, accTrain, i_batch // TEST_ITER)\n",
    "\n",
    "        if i_batch % SAVE_ITER == 0:\n",
    "            saver.save(sess, save_location)\n",
    "        \n",
    "        if i_batch % EPOCH == 0:\n",
    "            fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "            print('batch %r - loss: %r' % (i_batch, sess.run(loss, fd_test)))\n",
    "            predict_, target_ = sess.run([word_prediction, test_targets], fd_test)\n",
    "            for i, (inp, pred) in enumerate(zip(target_, predict_)):\n",
    "                print('    expected  > {}'.format(inp))\n",
    "                print('    predicted > {}'.format(pred))\n",
    "                if i >= 1:\n",
    "                    break\n",
    "            print()\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    saver.save(sess, save_location)\n",
    "    print('Training interrupted, model saved.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    fd_test = test_iterator.next_feed(BATCH_SIZE)\n",
    "    predict_, target_ = sess.run([word_prediction, test_targets], fd_test)\n",
    "    for i, (inp, pred) in enumerate(zip(target_, predict_)):\n",
    "        print('    expected  > {}'.format(inp))\n",
    "        print('    predicted > {}'.format(pred))\n",
    "        if i >= 1:\n",
    "            break\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
